{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the datasets for size reduction\n",
    "\n",
    "For obvious size reasons, we need to filter out the datasets to a reasonable size before using them.\n",
    "\n",
    "We know already that we won't need all the videos from all categories. Indeed, we only need the \"scientific\" and \"artistic\" videos. So we can rule out categories that do not fall in these domains. Example of categories we can exclude are:\n",
    "- Gaming\n",
    "- People & Blogs\n",
    "- Sports\n",
    "- Autos & Vehicles\n",
    "- News & Politics\n",
    "- Pets & Animals\n",
    "- Nonprofits & Activism\n",
    "- Not categorized\n",
    "\n",
    "Which leaves us with these categories **(TO DISCUSS)**:\n",
    "- Music\n",
    "- Entertainment\n",
    "- Howto & Style\n",
    "- Education\n",
    "- Film and Animation\n",
    "- Science & Technology\n",
    "- Comedy\n",
    "- Travel & Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the video dataset: yt_metadata_en.jsonl.gz (~16 Go)\n",
    "Steps:\n",
    "1. Download the dataset\n",
    "2. Load the dataset by chunks\n",
    "3. Keep only videos falling in the said categories\n",
    "4. Save the preprocessed dataset using `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'Music',\n",
    "    'Entertainment',\n",
    "    'Howto & Style',\n",
    "    'Education',\n",
    "    'Film and Animation', \n",
    "    'Science & Technology',\n",
    "    'Comedy',\n",
    "    'Travel & Events'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with title and description: 4316331632\n",
      "without title and description: 1367646599\n",
      "before category filtering: 1367646599\n",
      "after category filtering: 648824351\n",
      "before year filtering: 648824351\n",
      "after year filtering: 69876813\n",
      "Chunk n°1 completed. Loaded 105026/2000000 videos after filtering.\n",
      "with title and description: 4120973806\n",
      "without title and description: 1356637204\n",
      "before category filtering: 1356637204\n",
      "after category filtering: 636251962\n",
      "before year filtering: 636251962\n",
      "after year filtering: 77071253\n",
      "Chunk n°2 completed. Loaded 110607/2000000 videos after filtering.\n",
      "with title and description: 4141894851\n",
      "without title and description: 1357848927\n",
      "before category filtering: 1357848927\n",
      "after category filtering: 612400736\n",
      "before year filtering: 612400736\n",
      "after year filtering: 76508494\n",
      "Chunk n°3 completed. Loaded 111517/2000000 videos after filtering.\n",
      "with title and description: 4192431665\n",
      "without title and description: 1332780830\n",
      "before category filtering: 1332780830\n",
      "after category filtering: 593796679\n",
      "before year filtering: 593796679\n",
      "after year filtering: 74142361\n",
      "Chunk n°4 completed. Loaded 111011/2000000 videos after filtering.\n",
      "with title and description: 4123296226\n",
      "without title and description: 1356899103\n",
      "before category filtering: 1356899103\n",
      "after category filtering: 656140764\n",
      "before year filtering: 656140764\n",
      "after year filtering: 81715095\n",
      "Chunk n°5 completed. Loaded 123119/2000000 videos after filtering.\n",
      "with title and description: 4081829145\n",
      "without title and description: 1369622624\n",
      "before category filtering: 1369622624\n",
      "after category filtering: 710628545\n",
      "before year filtering: 710628545\n",
      "after year filtering: 87896879\n",
      "Chunk n°6 completed. Loaded 127938/2000000 videos after filtering.\n",
      "with title and description: 4264915635\n",
      "without title and description: 1375759943\n",
      "before category filtering: 1375759943\n",
      "after category filtering: 576826954\n",
      "before year filtering: 576826954\n",
      "after year filtering: 68676227\n",
      "Chunk n°7 completed. Loaded 102454/2000000 videos after filtering.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\ADA\\dataset_preprocessing.ipynb Cellule 5\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ADA/dataset_preprocessing.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m dfs \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ADA/dataset_preprocessing.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m chunk_num \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/ADA/dataset_preprocessing.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m df_json \u001b[39min\u001b[39;00m pd\u001b[39m.\u001b[39mread_json(videos_metadata_filepath, compression\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minfer\u001b[39m\u001b[39m\"\u001b[39m, chunksize\u001b[39m=\u001b[39mchunk_size, lines\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ADA/dataset_preprocessing.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# remove title and description\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ADA/dataset_preprocessing.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mwith title and description: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m sys\u001b[39m.\u001b[39mgetsizeof(df_json))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ADA/dataset_preprocessing.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     df_json\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m , inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\gasto\\anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\io\\json\\_json.py:796\u001b[0m, in \u001b[0;36mJsonReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[39mif\u001b[39;00m lines:\n\u001b[0;32m    795\u001b[0m     lines_json \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_lines(lines)\n\u001b[1;32m--> 796\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_object_parser(lines_json)\n\u001b[0;32m    798\u001b[0m     \u001b[39m# Make sure that the returned objects have the right index.\u001b[39;00m\n\u001b[0;32m    799\u001b[0m     obj\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnrows_seen, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnrows_seen \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(obj))\n",
      "File \u001b[1;32mc:\\Users\\gasto\\anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\io\\json\\_json.py:768\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    766\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 768\u001b[0m     obj \u001b[39m=\u001b[39m FrameParser(json, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mparse()\n\u001b[0;32m    770\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mseries\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    771\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\gasto\\anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\io\\json\\_json.py:880\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    878\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_numpy()\n\u001b[0;32m    879\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 880\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_no_numpy()\n\u001b[0;32m    882\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    883\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gasto\\anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\io\\json\\_json.py:1132\u001b[0m, in \u001b[0;36mFrameParser._parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1129\u001b[0m orient \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient\n\u001b[0;32m   1131\u001b[0m \u001b[39mif\u001b[39;00m orient \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 1132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m DataFrame(\n\u001b[0;32m   1133\u001b[0m         loads(json, precise_float\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecise_float), dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m   1134\u001b[0m     )\n\u001b[0;32m   1135\u001b[0m \u001b[39melif\u001b[39;00m orient \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1136\u001b[0m     decoded \u001b[39m=\u001b[39m {\n\u001b[0;32m   1137\u001b[0m         \u001b[39mstr\u001b[39m(k): v\n\u001b[0;32m   1138\u001b[0m         \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m loads(json, precise_float\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecise_float)\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1139\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\gasto\\anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\core\\frame.py:705\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n\u001b[1;32m--> 705\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39;49m(data, (abc\u001b[39m.\u001b[39mSequence, ExtensionArray)):\n\u001b[0;32m    706\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39m__array__\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    707\u001b[0m             \u001b[39m# GH#44616 big perf improvement for e.g. pytorch tensor\u001b[39;00m\n\u001b[0;32m    708\u001b[0m             data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(data)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "videos_metadata_filepath = 'data/yt_metadata_en.jsonl.gz'\n",
    "\n",
    "max_chunk_count = 0     # number of chunk to load (0 to load all the chunks)\n",
    "chunk_size = 2000000     # number of rows to load each chunk\n",
    "\n",
    "file_num = 1\n",
    "\n",
    "year = '2016'\n",
    "\n",
    "dfs = []\n",
    "chunk_num = 1\n",
    "for df_json in pd.read_json(videos_metadata_filepath, compression=\"infer\", chunksize=chunk_size, lines=True):\n",
    "    # remove title and description\n",
    "    print('with title and description: %d' % sys.getsizeof(df_json))\n",
    "    df_json.drop(['description', 'title'], axis=1 , inplace=True)\n",
    "    print('without title and description: %d' % sys.getsizeof(df_json))\n",
    "\n",
    "    # filter to keep only needed categories\n",
    "    print('before category filtering: %d' % sys.getsizeof(df_json))\n",
    "    df_json = df_json[df_json.categories.isin(categories)]\n",
    "    print('after category filtering: %d' % sys.getsizeof(df_json))\n",
    "\n",
    "    # filter to keep only needed categories\n",
    "    print('before year filtering: %d' % sys.getsizeof(df_json))\n",
    "    df_json = df_json[df_json.upload_date.str.startswith(year)]\n",
    "    print('after year filtering: %d' % sys.getsizeof(df_json))\n",
    "    \n",
    "\n",
    "\n",
    "    dfs.append(df_json)\n",
    "    print('Chunk n°%d completed. Loaded %d/%d videos after filtering.' % (chunk_num, len(df_json), chunk_size))\n",
    "\n",
    "    # if chunk_num % 10 == 0:\n",
    "    #     video_metadata = pd.concat(dfs)\n",
    "    #     dfs = []\n",
    "    #     filename = 'filtered/video_metadata%d' % file_num\n",
    "    #     print('Saving %d rows (%d Bytes) to %s' % (len(video_metadata), sys.getsizeof(video_metadata), filename))\n",
    "    #     pickle.dump(video_metadata, open(filename, 'wb'))\n",
    "    #     file_num += 1\n",
    "\n",
    "    #     # clear memory\n",
    "    #     %xdel video_metadata\n",
    "\n",
    "    chunk_num += 1\n",
    "    if chunk_num == max_chunk_count + 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadata = pd.concat(dfs).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 791672 rows (542219634 Bytes) to filtered/video_metadata.feather\n"
     ]
    }
   ],
   "source": [
    "filename = 'filtered/video_metadata.feather'\n",
    "print('Saving %d rows (%d Bytes) to %s' % (len(video_metadata), sys.getsizeof(video_metadata), filename))\n",
    "video_metadata.to_feather(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(video_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(video_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(video_metadata, open('video_metadata', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadata = pickle.load(open('video_metadata', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get memory usage\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "memory_usage = sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n",
    "# display the 3 most memory consuming variables\n",
    "memory_usage[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory for further analysis\n",
    "%xdel df_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the comments dataset: youtube_comments.tsv.gz (~70 Go)\n",
    "1. Download the dataset\n",
    "2. Load the dataset by chunks\n",
    "3. Only keep comments from videos present in the preprocessed dataset above\n",
    "4. Save the preprocessed dataset using `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ada')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97db0c6df4c7a8cbba01b62acaa799f28c3d2e725dabb47da6f81a32a81ae439"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
